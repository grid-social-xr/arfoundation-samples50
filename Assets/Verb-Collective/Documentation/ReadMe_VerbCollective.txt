Basic Documentation for Verb Collective

The Verb Collective is a toolkit designed to aid artists in exploring interactive media and to help non specialized instructors teach virtual and augmented reality.  It is designed to fit seamlessly into the interface of Unity, though it leverages some of the same structures of visual coding environments, it is intentionally designed without specialized interfaces. 

Verbs are short scripts that have very targeted functions.  Verbs, when triggered, can trigger other verbs on the original gameobject or else trigger verbs on a separate gameobject. Trigger verbs are used to begin interaction chains (example: whenTyped is used to trigger an array of verbs when a character on the keyboard is pressed) while standard verbs are set to wait for a trigger or else activate on start.  All types of verbs can be used to trigger an array of verbs at the end of their sequence.

The Verb Collective is designed to be modular and lightweight and for student expertise in its functions to be directly transferable to the Unity platform as well as other similar platforms. While other popular toolkits are focused either on rapid production or on teaching code itself, the Verb Collective is focused on emergent dynamics and fostering exploratory play. 

The modularity of the system is such that while it is limited in scope it is highly customizable. Designed with three tiers of users in mind, it functions as well for beginners as it does for more skilled developers. Using templates such as toSample, users can make their own custom verbs in minutes.  Beginers that want to write their own code from scratch can use the Verb Collective to grab chunks of basic code to copy and paste rather than relying on the internet for those things.

Included in the package are a number of additional assets providing tools for the quick creation of sonic environments and simple textures.  The idea is that if you were to only download the verb collective, you would be able to make expressive interfaces with visual and sonic elements.  There are also a number of 'starter' rooms that exist to provide venues for testing physics interactions, such as a bouncy room, a deep well, and a high plateau.  Again, the idea is that new users can focus on exploring interactions quickly, and begin customizing and fine tuning scenes once they are more comfortable with the interface.

This toolkit was designed at Yale University with support from HP as part of the Blended Reality Grant.  Special thanks go to Gus Schmedlen, Vice President of Worldwide Education at HP and Randall Rode from Yale University, whose combined support has been fundamental to our success.  This current package is our working beta and we hope to expand the range of verbs and improve the system with feedback from users.

For a more in depth description of the core strategies that are possible please see the complete paper, included in this package as a PDF.

Additionally included is a PDF with descriptions of every script.

Credits:
Justin Berry (Lead Developer)
Bobby Berry (original code base) - no relation :)
Shayne McGregor (code support, documentation, and testing)
Sam Lopate (audio support)
Charlie Orr (code support)

Verb Descriptions Online:
https://docs.google.com/document/d/1Ggje5ZcT32ByPmtGhPvyUOPUVKjUVcceM8_VKt1tQ38/edit?usp=sharing

More Info:
http://blendedreality.yale.edu/

Basic Intro Videos:
Part 1: https://www.youtube.com/watch?v=pwKqpxEo1oQ&t=8s
Part 2: https://www.youtube.com/watch?v=meckdDES0gE

Customer Support Contact:
verbcollectivecommunity@gmail.com

